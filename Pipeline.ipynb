{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import check_random_state\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMNIST():\n",
    "    t0 = time.time()\n",
    "    train_samples = 60000\n",
    "    # Load data from https://www.openml.org/d/554\n",
    "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "    X = X.reshape((X.shape[0], -1))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000)\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"==================\")\n",
    "    X_train=list(X_train)\n",
    "    X_test=list(X_test)\n",
    "    print(len(X_train))\n",
    "    print(len(X_test))\n",
    "    print(\"==================\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_38_Subset(X_train, y_train, X_test, y_test):\n",
    "    templist = []\n",
    "    for tup in zip(X_train, y_train):\n",
    "        if(tup[1]=='8' or tup[1]=='3'):\n",
    "            templist.append(list(tup))\n",
    "\n",
    "    testList = []\n",
    "    for tup in zip(X_test, y_test):\n",
    "        if(tup[1]=='8' or tup[1]=='3'):\n",
    "            testList.append(list(tup))\n",
    "\n",
    "    print(\"3/8 Training set size = \"+str(len(templist)))\n",
    "    print(\"3/8 Test set size = \"+str(len(testList)))\n",
    "    \n",
    "    X_test=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for tup in testList:\n",
    "        X_test.append(tup[0])\n",
    "        y_test.append(tup[1])\n",
    "\n",
    "\n",
    "    seed_size=int(0.1*len(templist))\n",
    "    seed_list=templist[0:seed_size]\n",
    "    print(\"Initial Seed set size = \"+str(len(seed_list)))\n",
    " \n",
    "    unlabelled_list=templist[seed_size:]\n",
    "    print(\"Unlabelled dataset size = \"+str(len(unlabelled_list)))\n",
    "    \n",
    "    return seed_list, unlabelled_list, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_instance_generate(seed_set, unlabelled_list, batch_size):\n",
    "    random_elements=[]\n",
    "    print(\"Before Sampling Unlabelled Data Size: \"+str(len(unlabelled_list)))\n",
    "    print(\"Before Sampling Seed Data Size: \"+str(len(seed_set)))\n",
    "    random.shuffle(unlabelled_list)\n",
    "    for i in range(batch_size):\n",
    "        rand_elem = random.choice(unlabelled_list)\n",
    "        random_elements.append(rand_elem)\n",
    "    for elem in random_elements:\n",
    "        seed_set.append(elem)\n",
    "        ul=[]\n",
    "    for i in random_elements:\n",
    "        ul.append(list(i[0]))\n",
    "        \n",
    "    new_unlabelled=[]\n",
    "    for elem in unlabelled_list:\n",
    "        if(list(elem[0]) in ul):\n",
    "            continue\n",
    "        new_unlabelled.append(elem)\n",
    "    print(\"After Sampling Unlabelled Data Size : \"+str(len(new_unlabelled)))\n",
    "    print(\"After Sampling Seed Data Size : \"+str(len(seed_set)))\n",
    "    return new_unlabelled, seed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split unlabeled into del_s (new points to add to S) and points removed from U (a new list)\n",
    "def transform_u(u, selected):\n",
    "    modified_u = [] # modified unlabeled\n",
    "    del_s = [] # new points to add to s\n",
    "    \n",
    "    i=0\n",
    "    for x in u:\n",
    "        if i in selected:\n",
    "            del_s.append(x)\n",
    "        else:\n",
    "            modified_u.append(x)            \n",
    "        i=i+1\n",
    "    return del_s, modified_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "def select_random_from_unlabeled(u, batch_size, model=None):\n",
    "    selected = set(npr.choice(len(u), batch_size, replace=False)) #these will no longer be unlabeled\n",
    "    return transform_u(u, selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare pairs of the form (id, value)\n",
    "import functools\n",
    "\n",
    "#descending order\n",
    "def compare_by_val(u, v):\n",
    "    if u[1] < v[1]:\n",
    "        return 1\n",
    "    elif u[1] == v[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    \n",
    "# the max value of the predicted probabilities = 1/num_classes\n",
    "# so if this value is small - it denotes a level of certainty\n",
    "# maximize these values for the uncertainty sampling\n",
    "def compute_uncertainty(predicted_probs):\n",
    "    return prod(predicted_probs)\n",
    "    \n",
    "def select_most_uncertain_from_unlabeled(u, batch_size, model):    \n",
    "    #compute probs\n",
    "    avp_list = []\n",
    "    i = 0\n",
    "    for x, y in u:\n",
    "        prob_wts = model.predict_proba([x])\n",
    "        p = compute_uncertainty(prob_wts[0]) # to be maximized\n",
    "        avp_list.append([i, p])\n",
    "        i=i+1\n",
    "    \n",
    "    #sort\n",
    "    sorted(avp_list, key=functools.cmp_to_key(compare_by_val))\n",
    "    \n",
    "    #select top ones - most uncertain\n",
    "    selected = []\n",
    "    i = 0\n",
    "    for i, p_i in avp_list:\n",
    "        selected.append(i)\n",
    "        i=i+1\n",
    "        if i==batch_size:\n",
    "            break\n",
    "        \n",
    "    return transform_u(u, selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, X_test, y_test):\n",
    "    #print(\"--------------------Start------------------\")\n",
    "    \n",
    "    clf = LogisticRegression(C=0.01, penalty='l2', solver='saga', tol=0.1)    \n",
    "    #print(\"Training with seed size: \" + str(len(data)))\n",
    "    \n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    for i in range(len(data)):\n",
    "        try:\n",
    "            X_train.append(list(data[i][0]))\n",
    "            y_train.append(data[i][1])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #print(\"Accuracy on the Test Set is : \"+str(sklearn.metrics.accuracy_score(y_test, clf.predict(X_test))))\n",
    "    #print(\"--------------------End------------------\")\n",
    "    return(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making MNIST data...\n",
      "==================\n",
      "60000\n",
      "10000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "print (\"Making MNIST data...\")\n",
    "X_train, y_train, X_test, y_test = getMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting MNIST 3/8 classification data...\n",
      "3/8 Training set size = 11987\n",
      "3/8 Test set size = 1979\n",
      "Initial Seed set size = 1198\n",
      "Unlabelled dataset size = 10789\n",
      "usample, 0 = 0.9191510864072764\n",
      "usample, 1 = 0.9307731177362304\n",
      "usample, 2 = 0.9363314805457301\n",
      "random, 0 = 0.9398686205154119\n",
      "random, 1 = 0.9413845376452754\n",
      "random, 2 = 0.9464375947448206\n"
     ]
    }
   ],
   "source": [
    "print (\"Getting MNIST 3/8 classification data...\")\n",
    "s, u, X_test, y_test = make_38_Subset(X_train, y_train, X_test, y_test)\n",
    "\n",
    "#Define the available selector functions\n",
    "selector_functions = {\n",
    "  'usample': select_most_uncertain_from_unlabeled,\n",
    "  'random': select_random_from_unlabeled\n",
    "}\n",
    "\n",
    "results = {}\n",
    "niters = 3\n",
    "batch_size = 1000\n",
    "\n",
    "for selector_function in selector_functions.keys():\n",
    "    #print (\"Results with: {}\".format(selector_function))\n",
    "    \n",
    "    for i in range(niters):\n",
    "        # print(\"|S|_{} = {}, |U|_{} = {}\".format(i, len(s), i, len(u)))\n",
    "        model = train_model(s, X_test, y_test)\n",
    "        del_s, u = selector_functions[selector_function](u, batch_size, model)    \n",
    "        s = s + del_s\n",
    "        \n",
    "        key = selector_function + \", \" + str(i)\n",
    "        acc_val = str(sklearn.metrics.accuracy_score(y_test, model.predict(X_test)))\n",
    "        results[key] = acc_val\n",
    "\n",
    "for key in results.keys():\n",
    "    print (\"{} = {}\".format(key, results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
